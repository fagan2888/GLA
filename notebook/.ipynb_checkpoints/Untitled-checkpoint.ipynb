{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of London spendings with python\n",
    "\n",
    "In this post I want to discuss how you can use python to fetch data from the internet, \n",
    "put them in a readable format and gain some interesting insights.\n",
    "\n",
    "This exercise is motivated by [\"Using SQL for Lightweight Data Analysis\"](http://schoolofdata.org/2013/03/26/using-sql-for-lightweight-data-analysis/) by Rufus Pollock. Here, I extend Rufus' analysis to a larger dataset and I use different analysis tools.\n",
    "\n",
    "## The data\n",
    "\n",
    "The data come from the [\"London GLA spending\"](https://www.london.gov.uk/about-us/greater-london-authority-gla/spending-money-wisely/our-spending) website, where GLA stands for Greater London Authority. Every month GLA publishes their spendings on Housing Services, Developing, Communities & Intelligence, etc. While writing, the GLA webpage contains 38 csv files with inhomogeneous formatting. There are empty columns and irregularly spaced data. To complicate things, the GLA website keeps changing root address and html design. So, I do not guarantee that the code described below will work in 2 years from now.\n",
    "\n",
    "The webpage looks like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vincepota/GLA/master/notebook/web_screenshot.png\" width=\"600\">\n",
    "\n",
    "where we are interested in the content of the `CSV file` column. \n",
    "\n",
    "The strategy is straightforward: \n",
    "- scrap the html code of the GLA webpage;\n",
    "- extract the links to the `.csv` files;\n",
    "- download all the data and append the results to a `pandas` dataframe;\n",
    "- Clean the data\n",
    "- Have some fun with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code\n",
    "\n",
    "Let's import some libraries, where the most important is `BeautifulSoup` which allows to handle the html code hiding behind web pages. If you do not have `BeautifulSoup` installed, you can get it via `pip install BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import matplotlib.pylab as plt\n",
    "import re\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the `html` code from the GLA webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wpage= 'https://www.london.gov.uk/about-us/greater-london-authority-gla/spending-money-wisely/our-spending'\n",
    "\n",
    "req = urllib2.Request(wpage)\n",
    "page = urllib2.urlopen(req)\n",
    "soup = BeautifulSoup(page, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `csv` files that we need are contained in `<td>` tags, which are nested inside `<table>` tags.\n",
    "Some `<td>` tags contain the direct link to the `csv` file, while other `<td>` tags contain a *link* to another webpage which contains the `csv` file. It is rather confusing, but it can be implemented very easily with python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = soup.find_all('table')     # Find all <table> tags\n",
    "thelist = []                       \n",
    "\n",
    "for t in table:\n",
    "    if len(t.find_all('th')) > 0:  # Only select tables with csv files\n",
    "        for a in t.find_all('a', href=True):   # Find all hyperlinks in the table\n",
    "            thelink = 'https:' + a['href']     \n",
    "            if len(thelink) < 40:        # If True, thelink is a link to another webpage\n",
    "                                         # containing the csv file\n",
    "                    \n",
    "               req = urllib2.Request(thelink)    # Scrap thelink wepage\n",
    "               page = urllib2.urlopen(req)\n",
    "               soup = BeautifulSoup(page, 'html5lib')\n",
    "\n",
    "               aa = soup.find_all(href = re.compile('.csv'))[0] # Extract the csv file\n",
    "               thelink = aa['href']\n",
    "               thelist.append(thelink)   \n",
    "            else:                        # If the link is a link to the csv file, append the\n",
    "               thelist.append(thelink)   # results straight away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`thelist` is a list which contains all the direct links to the `csv` files. Note that we have not downloaded the data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'the list contains', len(thelist), 'csv files'\n",
    "thelist[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now download the data. Instead of downloading every `csv` files to disk, one can use `pandas` ability to read `csv` files straight from the internet. Before we do that, let's see how the data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: u'https://www.london.gov.uk/sites/default/files/mayors_250_report_-_2015-16_-_p12_-_combined.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d8d34dc2268b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthelist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: u'https://www.london.gov.uk/sites/default/files/mayors_250_report_-_2015-16_-_p12_-_combined.csv'"
     ]
    }
   ],
   "source": [
    "example = urllib2.urlopen(thelist[0]).read(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u't'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
