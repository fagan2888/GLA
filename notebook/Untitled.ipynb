{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of London spendings with python\n",
    "\n",
    "In this post I want to discuss how you can use python to fetch data from the internet, \n",
    "put them in a readable format and gain some interesting insights.\n",
    "\n",
    "This exercise is motivated by [\"Using SQL for Lightweight Data Analysis\"](http://schoolofdata.org/2013/03/26/using-sql-for-lightweight-data-analysis/) by Rufus Pollock. Here, I extend Rufus' analysis to a larger dataset and I use different analysis tools.\n",
    "\n",
    "## The data\n",
    "\n",
    "The data come from the [\"London GLA spending\"](https://www.london.gov.uk/about-us/greater-london-authority-gla/spending-money-wisely/our-spending) website, where GLA stands for Greater London Authority. Every month GLA publishes their spendings on Housing Services, Developing, Communities & Intelligence, etc. While writing, the GLA webpage contains 38 csv files with inhomogeneous formatting. There are empty columns and irregularly spaced data. To complicate things, the GLA website keeps changing root address and html design. So, I do not guarantee that the code described below will work in 2 years from now.\n",
    "\n",
    "The webpage looks like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vincepota/GLA/master/notebook/web_screenshot.png\" width=\"600\">\n",
    "\n",
    "where we are interested in the content of the `CSV file` column. \n",
    "\n",
    "The strategy is straightforward: \n",
    "- scrap the html code of the GLA webpage;\n",
    "- extract the links to the `.csv` files;\n",
    "- download all the data and append the results to a `pandas` dataframe;\n",
    "- Clean the data\n",
    "- Have some fun with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code\n",
    "\n",
    "Let's import some libraries, where the most important is `BeautifulSoup` which allows to handle the html code hiding behind web pages. If you do not have `BeautifulSoup` installed, you can get it via `pip install BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import matplotlib.pylab as plt\n",
    "import re\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the `html` code from the GLA webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wpage= 'https://www.london.gov.uk/about-us/greater-london-authority-gla/spending-money-wisely/our-spending'\n",
    "\n",
    "req = urllib2.Request(wpage)\n",
    "page = urllib2.urlopen(req)\n",
    "soup = BeautifulSoup(page, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `csv` files that we need are contained in `<td>` tags, which are nested inside `<table>` tags.\n",
    "Some `<td>` tags contain the direct link to the `csv` file, while other `<td>` tags contain a *link* to another webpage which contains the `csv` file. It is rather confusing, but it can be implemented very easily with python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = soup.find_all('table')     # Find all <table> tags\n",
    "thelist = []                       \n",
    "\n",
    "for t in table:\n",
    "    if len(t.find_all('th')) > 0:  # Only select tables with csv files\n",
    "        for a in t.find_all('a', href=True):   # Find all hyperlinks in the table\n",
    "            thelink = 'https:' + a['href']     \n",
    "            if len(thelink) < 40:        # If True, thelink is a link to another webpage\n",
    "                                         # containing the csv file\n",
    "                    \n",
    "               req = urllib2.Request(thelink)    # Scrap thelink wepage\n",
    "               page = urllib2.urlopen(req)\n",
    "               soup = BeautifulSoup(page, 'html5lib')\n",
    "\n",
    "               aa = soup.find_all(href = re.compile('.csv'))[0] # Extract the csv file\n",
    "               thelink = aa['href']\n",
    "               thelist.append(thelink)   \n",
    "            else:                        # If the link is a link to the csv file, append the\n",
    "               thelist.append(thelink)   # results straight away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`thelist` is a list which contains all the direct links to the `csv` files. Note that we have not downloaded the data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the list contains 38 csv files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'https://www.london.gov.uk/sites/default/files/mayors_250_report_-_2015-16_-_p12_-_combined.csv',\n",
       " u'https://www.london.gov.uk/sites/default/files/mayors_250_report_-_2015-16_-_p11_-_combined_fn.csv',\n",
       " u'https://www.london.gov.uk/sites/default/files/mayors_250_report_-_2015-16_-_p10_-_combined.csv',\n",
       " u'https://www.london.gov.uk/sites/default/files/mayors_250_report_-_2015-16_-_p09_-_combined.csv',\n",
       " u'https://www.london.gov.uk/sites/default/files/copy_of_mayors_250_report_-_2015-16_-_p08_-.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'the list contains', len(thelist), 'csv files'\n",
    "thelist[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now download the data. Instead of downloading every `csv` files to disk, one can use `pandas` ability to read `csv` files straight from the internet. Before we do that, let's see how the data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = urllib2.urlopen(thelist[0]).read(20000)\n",
    "example = example.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',,,,,,,,,,\\r',\n",
       " ',Report of all payments made by GLA & GLA Land & Property for value equal to or greater than \\xa3 250.00 Excl. VAT,,,,,,,,,\\r',\n",
       " ',Reporting Period : ,12,,,,,,,,\\r',\n",
       " ',Start Date:,07 February 2016,,,,,,,,\\r',\n",
       " ',End Date:,05 March 2016,,,,,,,,\\r',\n",
       " ',Financial Year :,2015 / 16,,,,,,,,\\r',\n",
       " ',,,Amount \\xa3,,,,,,,\\r',\n",
       " ',Total Remuneration for the month,,\"3,086,689.45\",,,,,,,\\r',\n",
       " ',Irrecoverable VAT,,0.00,,,,,,,\\r',\n",
       " ',,,,,,,,,,\\r',\n",
       " ',Vendor ID,Vendor Name,Cost Element,Expenditure Account Code Description,Document No,Amount,Clearing Date,Directorate,Service Expenditure Analysis,\\r',\n",
       " ',10016524,TRANSPORT FOR LONDON,544071,FUNCTIONAL BODY GRANT PAYMENT,CHAPS649,\"66,253,087.00\",24 Feb  2016,RESOURCES,Highways and transport services,\\r',\n",
       " ',10016524,TRANSPORT FOR LONDON,544093,NLE - GRANT PMT TO TFL,CHAPS627,\"20,945,312.00\",15 Feb  2016,RESOURCES,Highways and transport services,\\r',\n",
       " ',NC,DCLG,544073,BUSINESS RATE RETENTION-CLG,CHAPS643,\"17,926,156.00\",22 Feb  2016,RESOURCES,Highways and transport services,\\r',\n",
       " ',15500400,LONDON BOROUGH OF SOUTHWARK,544075,GRANTS TO EXTERNAL ORGANISATIONS,5108675974,\"7,155,497.00\",24 Feb  2016,Housing,Capital,\\r',\n",
       " ',15500235,LONDON BOROUGH OF TOWER HAMLETS,544075,GRANTS TO EXTERNAL ORGANISATIONS,5108678522,\"3,357,407.00\",24 Feb  2016,Housing,Capital,\\r',\n",
       " ',10023412,POCKET LIVING 2013 LLP,544075,GRANTS TO EXTERNAL ORGANISATIONS,5108679428,\"1,702,760.19\",08 Feb  2016,Housing,Capital,\\r',\n",
       " ',10007096,R B KINGSTON UPON THAMES,544075,GRANTS TO EXTERNAL ORGANISATIONS,5108685297,\"1,685,169.00\",15 Feb  2016,Housing,Capital,\\r',\n",
       " ',18000025,HEXAGON HOUSING ASSOCIATION LIMITED,544076,HSG GRANTS TO REGISTERED PROVIDERS,1900034890,\"576,810.00\",19 Feb  2016,Housing,Capital,\\r',\n",
       " ',18000023,OCTAVIA HOUSING AND CARE,544076,HSG GRANTS TO REGISTERED PROVIDERS,1900034923,\"526,333.00\",25 Feb  2016,Housing,Capital,\\r']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
